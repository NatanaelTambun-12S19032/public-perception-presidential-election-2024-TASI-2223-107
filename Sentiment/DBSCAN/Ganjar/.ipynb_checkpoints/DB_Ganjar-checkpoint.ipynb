{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69c87c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# install package\n",
    "!pip -q install sastrawi\n",
    "!pip -q install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731abdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\natan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import chi2 \n",
    "import random\n",
    "from joblib import dump\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3435427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "def load_data():\n",
    "    data = pd.read_csv('E:/Documents/Semester 8/TAmbun/TA_FIX/K-Means++/Ganjar/Ganjar.csv' , sep='\\t', encoding='ISO-8859-1')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9c27196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Setuju pak Ganjar Pranowo presiden berikutnya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ikut aja , yg penting Indonesia aman damai , e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Duwa duwanya bagus,tapi yang muda ngalah dulu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Setuju pak Ginanjar Presidennya â¤ï¸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kalo saya Prabowo Presidennya Ganjar Wapresnya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27272</th>\n",
       "      <td>Dalan Semarang pak... Dr pedurungan ke penggar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27273</th>\n",
       "      <td>Nuwun sewu pak @ganjar_pranowo , jalan colomad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27274</th>\n",
       "      <td>mantapp pak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27275</th>\n",
       "      <td>â¤ï¸â¤ï¸â¤ï¸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27276</th>\n",
       "      <td>Asiap Pak ðð</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27277 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment\n",
       "0      Setuju pak Ganjar Pranowo presiden berikutnya ...\n",
       "1      Ikut aja , yg penting Indonesia aman damai , e...\n",
       "2      Duwa duwanya bagus,tapi yang muda ngalah dulu ...\n",
       "3                 Setuju pak Ginanjar Presidennya â¤ï¸\n",
       "4      Kalo saya Prabowo Presidennya Ganjar Wapresnya...\n",
       "...                                                  ...\n",
       "27272  Dalan Semarang pak... Dr pedurungan ke penggar...\n",
       "27273  Nuwun sewu pak @ganjar_pranowo , jalan colomad...\n",
       "27274                                        mantapp pak\n",
       "27275                                 â¤ï¸â¤ï¸â¤ï¸\n",
       "27276                                 Asiap Pak ðð\n",
       "\n",
       "[27277 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data()\n",
    "df = data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02e7bfc",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbb2e9ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m         text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m----> 7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_comment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_pattern\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m *RT* | *@[\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mw]*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2329\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m     vargs \u001b[38;5;241m=\u001b[39m [args[_i] \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m inds]\n\u001b[0;32m   2327\u001b[0m     vargs\u001b[38;5;241m.\u001b[39mextend([kwargs[_n] \u001b[38;5;28;01mfor\u001b[39;00m _n \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[1;32m-> 2329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vectorize_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2412\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[1;34m(self, func, args)\u001b[0m\n\u001b[0;32m   2409\u001b[0m \u001b[38;5;66;03m# Convert args to object arrays first\u001b[39;00m\n\u001b[0;32m   2410\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [asanyarray(a, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m-> 2412\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mnout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2415\u001b[0m     res \u001b[38;5;241m=\u001b[39m asanyarray(outputs, dtype\u001b[38;5;241m=\u001b[39motypes[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m, in \u001b[0;36mremove_pattern\u001b[1;34m(text, pattern_regex)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_pattern\u001b[39m(text, pattern_regex):\n\u001b[1;32m----> 3\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern_regex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m r:\n\u001b[0;32m      5\u001b[0m         text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "File \u001b[1;32mC:\\Program Files\\Python39\\lib\\re.py:241\u001b[0m, in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    234\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m \n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# remove mention username dan retweet\n",
    "def remove_pattern(text, pattern_regex):\n",
    "    r = re.findall(pattern_regex, text)\n",
    "    for i in r:\n",
    "        text = re.sub(i, '', text)\n",
    "    return text\n",
    "df['clean_comment']= np.vectorize(remove_pattern)(df['comment'], \" *RT* | *@[\\w]*\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf1ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove simbol/character\n",
    "def remove(text):\n",
    "    text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "    return text\n",
    "df['remove_http'] = df['clean_comment'].apply(lambda x: remove(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f41d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove\n",
    "def remove(tweet):\n",
    "    #remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "\n",
    "    #remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    #remove hastags, only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    #remove angka\n",
    "    tweet = re.sub('[0-9]+', '', tweet)\n",
    "\n",
    "    return tweet\n",
    "df['remove_hastag'] = df['remove_http'].apply(lambda x: remove(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de1ab0",
   "metadata": {},
   "source": [
    "#### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46cf184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menambah kamus stopword yang belum ada di python sastrawi\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "\n",
    "stop_factory = StopWordRemoverFactory().get_stop_words()\n",
    "more_stopwords = [\n",
    "    'yg', 'utk', 'cuman', 'deh', 'Btw', 'tapi', 'gua', 'gue', 'lo', 'lu',\n",
    "    'kalo', 'trs', 'jd', 'nih', 'ntar', 'nya', 'lg', 'gk', 'ecusli', 'dpt',\n",
    "    'dr', 'kpn', 'kok', 'kyk', 'donk', 'yah', 'u', 'ya', 'ga', 'km', 'eh',\n",
    "    'sih', 'eh', 'bang', 'br', 'kyk', 'rp', 'jt', 'kan', 'gpp', 'sm', 'usah',\n",
    "    'mas', 'sob', 'thx', 'ato', 'jg', 'gw', 'wkwk', 'mak', 'haha', 'iy', 'k',\n",
    "    'tp', 'haha', 'dg', 'dri', 'duh', 'ye', 'wkwkwk', 'syg', 'btw',\n",
    "    'nerjemahan', 'gaes', 'guys', 'moga', 'kmrn', 'nemu', 'yukkk',\n",
    "    'wkwkw', 'klas', 'iu', 'ew', 'lho', 'sbnry', 'org', 'gtu', 'bwt',\n",
    "    'klrga', 'clau', 'lbh', 'cpet', 'ku', 'uke', 'mba', 'mas', 'sdh', 'kmrn',\n",
    "    'oi', 'spt', 'dlm', 'bs', 'krn', 'jgn', 'sapa', 'spt', 'sh', 'wakakaka',\n",
    "    'sihhh', 'hehe', 'ih', 'dgn', 'la', 'kl', 'ttg', 'mana', 'kmna', 'kmn',\n",
    "    'tdk', 'tuh', 'dah', 'kek', 'ko', 'pls', 'bbrp', 'pd', 'mah', 'dhhh',\n",
    "    'kpd', 'tuh', 'kzl', 'byar', 'si', 'sii', 'cm', 'sy', 'hahahaha', 'weh',\n",
    "    'dlu', 'tuhh'\n",
    "]\n",
    "\n",
    "data = stop_factory + more_stopwords\n",
    "dictionary = ArrayDictionary(data)\n",
    "stp = StopWordRemover(dictionary)\n",
    "\n",
    "df[\"Text_wo_stop\"] = df[\"remove_hastag\"].apply(lambda text: stp.remove(text))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import string\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# tokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# happy emoticon\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'X-p', 'Xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "])\n",
    "\n",
    "# sad emoticon\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "])\n",
    "\n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in data and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean\n",
    "df['comment_token'] = df['Text_wo_stop'].apply(lambda x: clean_tweets(x))\n",
    "df.head() # tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e9c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punct\n",
    "def remove_punct(text):\n",
    "    text = \" \".join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "df['comment'] = df['comment_token'].apply(lambda x: remove_punct(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d98d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove comment kosong\n",
    "df = df[df['comment']!='']\n",
    "\n",
    "# reset index\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128be73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove kolom\n",
    "df.drop(df.columns[[1,2,3,4,5]], axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecbfe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean data to file\n",
    "df.to_csv('E:/Documents/Semester 8/TAmbun/TA_FIX/TAK-MEANS/Anies/DataBersih.csv', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f17bcbc",
   "metadata": {},
   "source": [
    "### Clustering K-Means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467aedf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon_lexis(file_path):\n",
    "    words = []\n",
    "    scores = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = line.strip().split(' ')\n",
    "            if len(data) >= 2:\n",
    "                word = data[0].strip()\n",
    "                score = int(data[1].strip())\n",
    "                words.append(word)\n",
    "                scores.append(score)\n",
    "    return dict(zip(words, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b597c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memuat kamus sentimen\n",
    "lexicon = load_lexicon_lexis('E:/Documents/Semester 8/TAmbun/TA_FIX/TAK-MEANS/Anies/kamus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbba3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:/Documents/Semester 8/TAmbun/TA_FIX/TAK-MEANS/Anies/DataBersih.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5605682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tambahkan kolom untuk menyimpan skor sentiment\n",
    "df['sentiment_score'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796561a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# melakukan pengecekan kata dalam setiap dokumen dan menghitung skor sentiment\n",
    "for i, row in df.iterrows():\n",
    "    words = row['comment'].split()\n",
    "    score = sum([lexicon.get(word, 0) for word in words])\n",
    "    df.at[i, 'sentiment_score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebf8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fafa7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformasi tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2))\n",
    "X = tfidf_vectorizer.fit_transform(df['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a655a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# melakukan scaling pada data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b2d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_blobs(n_samples=len(df), centers=3, random_state=42)\n",
    "\n",
    "# Set the number of clusters to test\n",
    "k_range = range(1, 10)\n",
    "\n",
    "# Set the seed for the k-means++ algorithm\n",
    "seed = 42\n",
    "\n",
    "# List to store the SSE values for each k\n",
    "sse = []\n",
    "\n",
    "# Loop over k values and fit the k-means model\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=seed)\n",
    "    kmeans.fit(X)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the SSE values against k\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(k_range, sse)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5124ec82",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3602bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# melakukan clustering dengan jumlah cluster yang optimal\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
    "kmeans.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mendapatkan label kelompok untuk setiap data\n",
    "labels = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a880a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mendapatkan jumlah kelompok yang terbentuk\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(\"Jumlah kelompok: \", n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe928c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengubah data hasil clustering ke dalam format DataFrame\n",
    "df_clustered = pd.DataFrame({'comment': df['comment'], 'cluster': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e017e",
   "metadata": {},
   "source": [
    "##### Evaluation With Silhouette Coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de5da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9dff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample data\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedb69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghitung silhouette score\n",
    "score = silhouette_score(X, labels)\n",
    "print(\"Silhouette score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#menampilkan hasil clustering\n",
    "df['cluster'] = labels\n",
    "\n",
    "# Display the number of data points in each cluster\n",
    "print(df.groupby(['cluster']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# membuat DataFrame untuk data dan label kelompok\n",
    "df_cluster = pd.DataFrame(X, columns=['x1', 'x2'])\n",
    "df_cluster['cluster'] = labels\n",
    "\n",
    "# membuat plot menggunakan seaborn\n",
    "sns.scatterplot(x='x1', y='x2', data=df_cluster, hue='cluster', palette='bright')\n",
    "plt.title(f\"Clustering hasil k-means dengan {n_clusters} kelompok\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128ce4f0",
   "metadata": {},
   "source": [
    "##### Ekstrak 10 Top Kata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f17cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# menghitung jarak antara setiap dokumen dan centroid\n",
    "distances = euclidean_distances(X, kmeans.cluster_centers_.reshape(n_clusters, -1))\n",
    "\n",
    "# untuk setiap cluster yang terbentuk, ambil 10 top kata teratas dan identifikasi nilai sentimennya\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_words = []\n",
    "    for doc_id, label in enumerate(labels):\n",
    "        if label == cluster_id:\n",
    "            words = df.iloc[doc_id]['comment'].split()\n",
    "            cluster_words.extend(words)\n",
    "    top_words = pd.Series(cluster_words).value_counts().head(10)\n",
    "    top_words_scores = [lexicon.get(word, 0) for word in top_words.index]\n",
    "    print(f\"Cluster {cluster_id} Top 10 words: {', '.join(top_words.index)}\")\n",
    "    print(f\"Sentiment Scores: {', '.join(map(str, top_words_scores))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9264214",
   "metadata": {},
   "source": [
    "##### Menghitung Polarity Dari Setiap Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1596906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hitung rata-rata polarity dari setiap cluster\n",
    "cluster_sentiment = []\n",
    "for i in range(n_clusters):\n",
    "    cluster = df[df['cluster'] == i]\n",
    "    sentiment_sum = cluster['sentiment_score'].sum()\n",
    "    num_docs = len(cluster)\n",
    "    avg_sentiment = sentiment_sum / num_docs\n",
    "    cluster_sentiment.append(avg_sentiment)\n",
    "    print(\"Cluster \", i, \" avg sentiment: \", avg_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d5f31",
   "metadata": {},
   "source": [
    "##### Menampilkan Hasil Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd73cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat array sentimen posifif, netral, dan negatif\n",
    "pos_sentiments = np.arange(1, 6)\n",
    "neu_sentiments = np.array([0])\n",
    "neg_sentiments = np.arange(-5, 0)\n",
    "\n",
    "# Membuat fungsi untuk menghitung jumlah sentimen pada setiap cluster\n",
    "def count_sentiments(cluster):\n",
    "    pos_count = len([score for score in cluster if score in pos_sentiments])\n",
    "    neu_count = len([score for score in cluster if score in neu_sentiments])\n",
    "    neg_count = len([score for score in cluster if score in neg_sentiments])\n",
    "    return pos_count, neu_count, neg_count\n",
    "\n",
    "# Membuat list untuk menyimpan jumlah sentimen positif, netral, dan negatif pada setiap cluster\n",
    "sentiments = []\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    cluster = df[df['cluster'] == i]['sentiment_score']\n",
    "    pos_count, neu_count, neg_count = count_sentiments(cluster)\n",
    "    sentiments.append((pos_count, neu_count, neg_count))\n",
    "\n",
    "# Menampilkan jumlah sentimen positif, netral, dan negatif pada setiap cluster\n",
    "for i, sentiment in enumerate(sentiments):\n",
    "    print(f\"Cluster {i}: {sentiment[0]} sentimen positif, {sentiment[1]} sentimen netral, {sentiment[2]} sentimen negatif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8143fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengakumulasi total jumlah sentimen positif, negatif, dan netral dari keseluruhan clusternya\n",
    "total_pos_count = sum([sentiment[0] for sentiment in sentiments])\n",
    "total_neu_count = sum([sentiment[1] for sentiment in sentiments])\n",
    "total_neg_count = sum([sentiment[2] for sentiment in sentiments])\n",
    "\n",
    "# Menampilkan total jumlah sentimen positif, netral, dan negatif dari keseluruhan clusternya\n",
    "print(f\"Total sentimen positif: {total_pos_count}\")\n",
    "print(f\"Total sentimen netral: {total_neu_count}\")\n",
    "print(f\"Total sentimen negatif: {total_neg_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effbd423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Menyiapkan data untuk pie chart\n",
    "labels = ['Positif', 'Netral', 'Negatif']\n",
    "sizes = [total_pos_count, total_neu_count, total_neg_count]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#d62728']\n",
    "\n",
    "# Membuat pie chart\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Menambahkan judul pada pie chart\n",
    "ax.set_title('Total Sentimen pada Seluruh Cluster')\n",
    "\n",
    "# Menampilkan pie chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad6587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
